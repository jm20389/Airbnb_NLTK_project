{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlf-VjoOZ8O"
   },
   "source": [
    "# Part 3 - Text analysis and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU8BnCXIOZ8T"
   },
   "source": [
    "# 3.a Computing PMI\n",
    "\n",
    "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_BJYvjpOZ8U"
   },
   "source": [
    "### Imports, data loading and helper functions\n",
    "\n",
    "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21015647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0z_s4GpwOZ8U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c21015647\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# These are the additional libraries I have included myself:\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFP8c6HlPF_-",
    "outputId": "0fa313c5-497c-44f6-f747-4d7ebf651661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\c21015647\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c21015647\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\c21015647\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9JOWJqE9Pq5V"
   },
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "# As there are reviews in spanish and french, we include the stopwords from those languages in the whole set (all_sw):\n",
    "\n",
    "sw_french = set(stopwords.words('french'))\n",
    "sw_spanish = set(stopwords.words('spanish'))\n",
    "all_sw = list(set(itertools.chain.from_iterable([sw, sw_french, sw_spanish])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LVD9Q3AxOZ8V"
   },
   "outputs": [],
   "source": [
    "p = 'data'\n",
    "df = pd.read_csv(os.path.join(p,'reviews.csv'))\n",
    "# deal with empty reviews\n",
    "df.comments = df.comments.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "pNgPCqMPOZ8V",
    "outputId": "dd74578a-59c0-45c0-9228-3fefd61ac153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id    id        date  reviewer_id reviewer_name  \\\n",
       "0        2818  1191  2009-03-30        10952           Lam   \n",
       "1        2818  1771  2009-04-24        12798         Alice   \n",
       "2        2818  1989  2009-05-03        11869       Natalja   \n",
       "3        2818  2797  2009-05-18        14064       Enrique   \n",
       "4        2818  3151  2009-05-25        17977       Sherwin   \n",
       "\n",
       "                                            comments  \n",
       "0  Daniel is really cool. The place was nice and ...  \n",
       "1  Daniel is the most amazing host! His place is ...  \n",
       "2  We had such a great time in Amsterdam. Daniel ...  \n",
       "3  Very professional operation. Room is very clea...  \n",
       "4  Daniel is highly recommended.  He provided all...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_9leP4VOZ8W",
    "outputId": "010fcf4a-300c-4749-8cb8-04bed1fe68cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452143, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfVvyXyPYS4"
   },
   "source": [
    "### 3.a1 - Process reviews\n",
    "\n",
    "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b7jF_XXsQYgK"
   },
   "outputs": [],
   "source": [
    "def process_reviews(df):\n",
    "  # your code here\n",
    "\n",
    "    '''\n",
    "    Takes as argument a dataframe containing a column of text reviews/user comments (strings) and returns the same dataframe\n",
    "    but with three extra natural language analysis columns, as described below.\n",
    "    \n",
    "    Args:\n",
    "        A DataFrame containing a column called \"comments\". This column should be of type string.\n",
    "        \n",
    "    Returns:\n",
    "        Original DataFrame but with three additional columns:\n",
    "        \n",
    "            tokenized: Comments column converted to list of strings, the words from original sentence (tokens).\n",
    "            tagged: tokenized column after applying Part-of-speech analysis (list of tagged tokens or tuples).\n",
    "            lower_tagged: tagged column with words converted to lowercase in order to reduce the vocabulary. \n",
    "            \n",
    "    '''\n",
    " \n",
    "    df['tokenized'] = df['comments'].progress_apply(word_tokenize)\n",
    "        \n",
    "    df['tagged'] = df['tokenized'].progress_apply(pos_tag)\n",
    "    \n",
    "    \n",
    "    # We convert to lowercase AFTER having tagged the words:\n",
    "    \n",
    "    lowerize_tagged = lambda sentence: [(tup[0].lower(), tup[1]) for tup in sentence]\n",
    "    \n",
    "    df['lower_tagged'] = df['tagged'].progress_apply(lowerize_tagged)\n",
    "    \n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rGYB8gx5Qq-P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 452143/452143 [03:15<00:00, 2309.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 452143/452143 [18:54<00:00, 398.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 452143/452143 [00:47<00:00, 9586.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tagged</th>\n",
       "      <th>lower_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818</td>\n",
       "      <td>1191</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>10952</td>\n",
       "      <td>Lam</td>\n",
       "      <td>Daniel is really cool. The place was nice and ...</td>\n",
       "      <td>[Daniel, is, really, cool, ., The, place, was,...</td>\n",
       "      <td>[(Daniel, NNP), (is, VBZ), (really, RB), (cool...</td>\n",
       "      <td>[(daniel, NNP), (is, VBZ), (really, RB), (cool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2818</td>\n",
       "      <td>1771</td>\n",
       "      <td>2009-04-24</td>\n",
       "      <td>12798</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Daniel is the most amazing host! His place is ...</td>\n",
       "      <td>[Daniel, is, the, most, amazing, host, !, His,...</td>\n",
       "      <td>[(Daniel, NNP), (is, VBZ), (the, DT), (most, R...</td>\n",
       "      <td>[(daniel, NNP), (is, VBZ), (the, DT), (most, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2818</td>\n",
       "      <td>1989</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>11869</td>\n",
       "      <td>Natalja</td>\n",
       "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
       "      <td>[We, had, such, a, great, time, in, Amsterdam,...</td>\n",
       "      <td>[(We, PRP), (had, VBD), (such, JJ), (a, DT), (...</td>\n",
       "      <td>[(we, PRP), (had, VBD), (such, JJ), (a, DT), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2818</td>\n",
       "      <td>2797</td>\n",
       "      <td>2009-05-18</td>\n",
       "      <td>14064</td>\n",
       "      <td>Enrique</td>\n",
       "      <td>Very professional operation. Room is very clea...</td>\n",
       "      <td>[Very, professional, operation, ., Room, is, v...</td>\n",
       "      <td>[(Very, RB), (professional, JJ), (operation, N...</td>\n",
       "      <td>[(very, RB), (professional, JJ), (operation, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2818</td>\n",
       "      <td>3151</td>\n",
       "      <td>2009-05-25</td>\n",
       "      <td>17977</td>\n",
       "      <td>Sherwin</td>\n",
       "      <td>Daniel is highly recommended.  He provided all...</td>\n",
       "      <td>[Daniel, is, highly, recommended, ., He, provi...</td>\n",
       "      <td>[(Daniel, NNP), (is, VBZ), (highly, RB), (reco...</td>\n",
       "      <td>[(daniel, NNP), (is, VBZ), (highly, RB), (reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2818</td>\n",
       "      <td>4748</td>\n",
       "      <td>2009-06-29</td>\n",
       "      <td>20192</td>\n",
       "      <td>Jie</td>\n",
       "      <td>Daniel was a great host! He made everything so...</td>\n",
       "      <td>[Daniel, was, a, great, host, !, He, made, eve...</td>\n",
       "      <td>[(Daniel, NNP), (was, VBD), (a, DT), (great, J...</td>\n",
       "      <td>[(daniel, NNP), (was, VBD), (a, DT), (great, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2818</td>\n",
       "      <td>5202</td>\n",
       "      <td>2009-07-07</td>\n",
       "      <td>23055</td>\n",
       "      <td>Vanessa</td>\n",
       "      <td>Daniele is an amazing host! He provided everyt...</td>\n",
       "      <td>[Daniele, is, an, amazing, host, !, He, provid...</td>\n",
       "      <td>[(Daniele, NNP), (is, VBZ), (an, DT), (amazing...</td>\n",
       "      <td>[(daniele, NNP), (is, VBZ), (an, DT), (amazing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2818</td>\n",
       "      <td>9131</td>\n",
       "      <td>2009-09-06</td>\n",
       "      <td>26343</td>\n",
       "      <td>Katja</td>\n",
       "      <td>You can´t have a nicer start in Amsterdam. Dan...</td>\n",
       "      <td>[You, can´t, have, a, nicer, start, in, Amster...</td>\n",
       "      <td>[(You, PRP), (can´t, VBP), (have, VB), (a, DT)...</td>\n",
       "      <td>[(you, PRP), (can´t, VBP), (have, VB), (a, DT)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2818</td>\n",
       "      <td>12103</td>\n",
       "      <td>2009-10-01</td>\n",
       "      <td>40999</td>\n",
       "      <td>Marie-Eve</td>\n",
       "      <td>Daniel was a fantastic host. His place is calm...</td>\n",
       "      <td>[Daniel, was, a, fantastic, host, ., His, plac...</td>\n",
       "      <td>[(Daniel, NNP), (was, VBD), (a, DT), (fantasti...</td>\n",
       "      <td>[(daniel, NNP), (was, VBD), (a, DT), (fantasti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2818</td>\n",
       "      <td>16196</td>\n",
       "      <td>2009-11-04</td>\n",
       "      <td>38623</td>\n",
       "      <td>Graham</td>\n",
       "      <td>Daniel was great. He couldn.t do enough for us...</td>\n",
       "      <td>[Daniel, was, great, ., He, couldn.t, do, enou...</td>\n",
       "      <td>[(Daniel, NNP), (was, VBD), (great, JJ), (., ....</td>\n",
       "      <td>[(daniel, NNP), (was, VBD), (great, JJ), (., ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id     id        date  reviewer_id reviewer_name  \\\n",
       "0        2818   1191  2009-03-30        10952           Lam   \n",
       "1        2818   1771  2009-04-24        12798         Alice   \n",
       "2        2818   1989  2009-05-03        11869       Natalja   \n",
       "3        2818   2797  2009-05-18        14064       Enrique   \n",
       "4        2818   3151  2009-05-25        17977       Sherwin   \n",
       "5        2818   4748  2009-06-29        20192           Jie   \n",
       "6        2818   5202  2009-07-07        23055       Vanessa   \n",
       "7        2818   9131  2009-09-06        26343         Katja   \n",
       "8        2818  12103  2009-10-01        40999     Marie-Eve   \n",
       "9        2818  16196  2009-11-04        38623        Graham   \n",
       "\n",
       "                                            comments  \\\n",
       "0  Daniel is really cool. The place was nice and ...   \n",
       "1  Daniel is the most amazing host! His place is ...   \n",
       "2  We had such a great time in Amsterdam. Daniel ...   \n",
       "3  Very professional operation. Room is very clea...   \n",
       "4  Daniel is highly recommended.  He provided all...   \n",
       "5  Daniel was a great host! He made everything so...   \n",
       "6  Daniele is an amazing host! He provided everyt...   \n",
       "7  You can´t have a nicer start in Amsterdam. Dan...   \n",
       "8  Daniel was a fantastic host. His place is calm...   \n",
       "9  Daniel was great. He couldn.t do enough for us...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Daniel, is, really, cool, ., The, place, was,...   \n",
       "1  [Daniel, is, the, most, amazing, host, !, His,...   \n",
       "2  [We, had, such, a, great, time, in, Amsterdam,...   \n",
       "3  [Very, professional, operation, ., Room, is, v...   \n",
       "4  [Daniel, is, highly, recommended, ., He, provi...   \n",
       "5  [Daniel, was, a, great, host, !, He, made, eve...   \n",
       "6  [Daniele, is, an, amazing, host, !, He, provid...   \n",
       "7  [You, can´t, have, a, nicer, start, in, Amster...   \n",
       "8  [Daniel, was, a, fantastic, host, ., His, plac...   \n",
       "9  [Daniel, was, great, ., He, couldn.t, do, enou...   \n",
       "\n",
       "                                              tagged  \\\n",
       "0  [(Daniel, NNP), (is, VBZ), (really, RB), (cool...   \n",
       "1  [(Daniel, NNP), (is, VBZ), (the, DT), (most, R...   \n",
       "2  [(We, PRP), (had, VBD), (such, JJ), (a, DT), (...   \n",
       "3  [(Very, RB), (professional, JJ), (operation, N...   \n",
       "4  [(Daniel, NNP), (is, VBZ), (highly, RB), (reco...   \n",
       "5  [(Daniel, NNP), (was, VBD), (a, DT), (great, J...   \n",
       "6  [(Daniele, NNP), (is, VBZ), (an, DT), (amazing...   \n",
       "7  [(You, PRP), (can´t, VBP), (have, VB), (a, DT)...   \n",
       "8  [(Daniel, NNP), (was, VBD), (a, DT), (fantasti...   \n",
       "9  [(Daniel, NNP), (was, VBD), (great, JJ), (., ....   \n",
       "\n",
       "                                        lower_tagged  \n",
       "0  [(daniel, NNP), (is, VBZ), (really, RB), (cool...  \n",
       "1  [(daniel, NNP), (is, VBZ), (the, DT), (most, R...  \n",
       "2  [(we, PRP), (had, VBD), (such, JJ), (a, DT), (...  \n",
       "3  [(very, RB), (professional, JJ), (operation, N...  \n",
       "4  [(daniel, NNP), (is, VBZ), (highly, RB), (reco...  \n",
       "5  [(daniel, NNP), (was, VBD), (a, DT), (great, J...  \n",
       "6  [(daniele, NNP), (is, VBZ), (an, DT), (amazing...  \n",
       "7  [(you, PRP), (can´t, VBP), (have, VB), (a, DT)...  \n",
       "8  [(daniel, NNP), (was, VBD), (a, DT), (fantasti...  \n",
       "9  [(daniel, NNP), (was, VBD), (great, JJ), (., ....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = process_reviews(df)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaH-yNlQRL9"
   },
   "source": [
    "### 3.a2 - Create a vocabulary\n",
    "\n",
    "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sAg6VRwdQQmg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "  # your code here\n",
    "\n",
    "    '''\n",
    "    Takes a dataframe containing a column been analyzed with part-of-speech, and returns the top 1000 most common words \n",
    "    for central and context vocabulary.\n",
    "    \n",
    "    Arguments:\n",
    "        df: Dataframe as described above.\n",
    "    \n",
    "    Returns:\n",
    "        cent_vocab: Top 1000 most common central words (nouns).\n",
    "        cont_vocab: Top 1000 most common context words (verbs or adjectives).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def noun_filter(tagged_list):\n",
    "        '''\n",
    "        Nested function to filter the tags from a pos-tagged sentence. Keeps only Nouns.\n",
    "        \n",
    "        Arguments:\n",
    "            tagged_list: list of tagged words (list of tuples, in the form (word, tag) ).\n",
    "            \n",
    "        Returns:\n",
    "            Filtered input list, having left only tagged words (tuples) whose tags start with 'N'\n",
    "        \n",
    "        '''        \n",
    "        return [tup[0] for tup in tagged_list if tup[1][0] == 'N' ]\n",
    "    \n",
    "    \n",
    "    def verb_adj_filter(tagged_list):\n",
    "        '''\n",
    "        Nested function to filter the tags from a pos-tagged sentence. Keep only Verbs or Adjectives.\n",
    "        \n",
    "        Arguments:\n",
    "            tagged_list: list of tagged words (list of tuples, in the form (word, tag) )\n",
    "            \n",
    "        Returns:\n",
    "            Filtered input list, having left only tagged words (tuples) whose tags start with 'V' or 'J'\n",
    "        \n",
    "        '''    \n",
    "        return [tup[0] for tup in tagged_list if (tup[1][0] == 'V' or tup[1][0] == 'J') ]\n",
    "    \n",
    "\n",
    "    \n",
    "            # Filter the tagged reviews series into Nouns and Verbs & Adjectives:\n",
    "        \n",
    "        \n",
    "    # We apply the above functions to filter the column lower_tagged into nouns_only and ver_adj_only.\n",
    "    \n",
    "    # To reduce the number of potential accidental captures, we apply a lambda function to remove stopwords.\n",
    "    # The function will also filter out words of length 1, to discard possible accidental punctuation captures.\n",
    "    remove_sw = lambda sentence : [word for word in sentence if ((not word in all_sw) and (len(word)>1))]\n",
    "    \n",
    "    df['nouns_only'] = df['lower_tagged'].progress_apply(noun_filter).progress_apply(remove_sw)\n",
    "    \n",
    "    # Repeat the same step for verbs and adjectives.\n",
    "   \n",
    "    df['verb_adj_only'] = df['lower_tagged'].progress_apply(verb_adj_filter).progress_apply(remove_sw)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Merge the above series into a single list, for either nouns or verb/adj :\n",
    "    \n",
    "    all_nouns = list(itertools.chain.from_iterable(df['nouns_only']))\n",
    "    all_verb_adj = list(itertools.chain.from_iterable(df['verb_adj_only']))\n",
    "    \n",
    "     \n",
    "    # Use Counter to obtain a dictionary of unique values and their frequency. \n",
    "    # Then CHAIN with most_common to keep the top 1000 words with higher frequency.\n",
    "    \n",
    "    cent_vocab = Counter(all_nouns).most_common(1000)\n",
    "    cont_vocab = Counter(all_verb_adj).most_common(1000)\n",
    "    \n",
    "    \n",
    "    # We have obtained a list of tuples in the form (word, frequency).\n",
    "    # We are only interested in the words, not the frequency. Filter to remove frequencies:\n",
    "    \n",
    "    cent_vocab = [word[0] for word in cent_vocab]\n",
    "    cont_vocab = [word[0] for word in cont_vocab]\n",
    "    \n",
    "    \n",
    "    return cent_vocab, cont_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F_R5l4IVSk9-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 452143/452143 [01:01<00:00, 7335.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 452143/452143 [02:02<00:00, 3678.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 452143/452143 [01:03<00:00, 7118.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 452143/452143 [01:41<00:00, 4446.58it/s]\n"
     ]
    }
   ],
   "source": [
    "cent_vocab, cont_vocab = get_vocab(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkqRGdQ_RUMg"
   },
   "source": [
    "### 3.a3 Count co-occurrences between center and context words\n",
    "\n",
    "With these two 1,000-word vocabularies, create a co-occurrence matrix where, for each center word, you keep track of how many of the context words co-occur with it. Consider this short review with only one sentence as an example, where we want to get co-occurrences for verbs and adjectives for the center word restaurant:\n",
    "\n",
    "a. ‘A big restaurant served delicious food in big dishes’\n",
    "{‘restaurant’: {‘big’: 2, ‘served’:1, ‘delicious’:1}}\n",
    "\n",
    "\n",
    "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ddnfCbQWRd5R"
   },
   "outputs": [],
   "source": [
    "def get_coocs(df, cent_vocab, cont_vocab):\n",
    "  # your code here\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with a column containing text entries, relative to the vocabulary.\n",
    "        cent_vocab: List of most common nouns.\n",
    "        cont_vocab: List of most common context words.\n",
    "        \n",
    "    Returns:\n",
    "        A DataFrame showing relative frequencies between nouns and their surrounding context words.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Create a default dictionary to start collecting keys (center words) as they turn up during the search.\n",
    "    # The default dictionary value is a list, that will collect all the context words for each key (center word).\n",
    "    \n",
    "    freqs = defaultdict(list)\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    To be honest the below decision has been made as a compromise between allowed computation time and sample evaluation of\n",
    "    user comments.\n",
    "    \n",
    "    Due to the lenght of the comments I considered a valid answer to evaluate the context as a full review. In this way,\n",
    "    two labmda functions have been defined to split the tokenized comments between central and context words contained on\n",
    "    each entry of the Data Frame. Duplicated entries of central words have been clustered using the set function.\n",
    "    \n",
    "    As the comments are tend to be concise and focused in a one or two topics, will associate the context words to the\n",
    "    central words appearing on each entry iterating by a loop through the whole Data Frame.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Create two additional columns, containing the centre and context words for each comment/entry in the dataframe:\n",
    "    \n",
    "    cent_vocab_filter = lambda comment : set([word for word in comment if word in cent_vocab])\n",
    "    cont_vocab_filter = lambda comment : [word for word in comment if word in cont_vocab]\n",
    "     \n",
    "    df['comments_cent_vocab'] = df['tokenized'].progress_apply(cent_vocab_filter)    \n",
    "    df['comments_cont_vocab'] = df['tokenized'].progress_apply(cont_vocab_filter)\n",
    "\n",
    "    \n",
    "    # Iterate through the dataframe and associate centre words with context words, for each comment/entry:\n",
    "    for index in tqdm(df.index):\n",
    "        \n",
    "        for cent_word in df.loc[index, 'comments_cent_vocab']:\n",
    "            freqs[cent_word] = freqs[cent_word] + df.loc[index, 'comments_cont_vocab']\n",
    "            \n",
    "        \n",
    "        \n",
    "    # At this point we've got a dictionary whose keys are the centre words and corresponding values the raw occurrences of\n",
    "    # context words around them. Now we apply the last step which is clustering repeating occurrences in nested dictionaries.\n",
    "        \n",
    "    coocs = defaultdict(dict)\n",
    "    \n",
    "    for center_word in tqdm(cent_vocab):\n",
    "        coocs[center_word] = dict(Counter(freqs[center_word]))\n",
    "            \n",
    "                                        \n",
    "    return coocs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTT_TOkaSoXL"
   },
   "outputs": [],
   "source": [
    "# I've been unable to apply the function to the whole dataframe due to excessively long computation times. <<<<<<<<<<\n",
    "\n",
    "# Tried and think about a more efficient/simple function but that was the most simple version I could find.\n",
    "# As a result, I will apply the get_coocs function to the first 10000 entries of the df only..\n",
    "\n",
    "coocs = get_coocs(df[:10000], cent_vocab, cont_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6mOXqMRlt-"
   },
   "source": [
    "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
    "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "C6WuM5U7RsBJ"
   },
   "outputs": [],
   "source": [
    "def cooc_dict2df(coocs):\n",
    "  # your code here\n",
    "\n",
    "    '''\n",
    "    Transforms a dictionary of dictionaries into a DataFrame showing co-occurrence values.\n",
    "    \n",
    "    Args:\n",
    "        coocs: Dictionary of dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "        coocdf.T : Transposed DataFrame of co-occurrence values.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Panda's DataFrame function takes the dictionary as input, uses the primary keys of the dictionary to build the columns.\n",
    "    # The nested keys from the inner-dictionaries will be taken as the index series for the dataframe.\n",
    "    \n",
    "    coocdf = pd.DataFrame(coocs).fillna(0)\n",
    "    \n",
    "    # The above DataFrame has been build with center words as columns and context words as rows.\n",
    "    # As the exercise asks to return the center words as rows, we transpose the DataFrame using the T function on the output.\n",
    "    \n",
    "    return coocdf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "cwAflxldSrbg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 998)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>nice</th>\n",
       "      <th>clean</th>\n",
       "      <th>quiet</th>\n",
       "      <th>use</th>\n",
       "      <th>didnt</th>\n",
       "      <th>finding</th>\n",
       "      <th>come</th>\n",
       "      <th>back</th>\n",
       "      <th>amazing</th>\n",
       "      <th>...</th>\n",
       "      <th>wenn</th>\n",
       "      <th>schöne</th>\n",
       "      <th>wirklich</th>\n",
       "      <th>noch</th>\n",
       "      <th>diese</th>\n",
       "      <th>jeden</th>\n",
       "      <th>sie</th>\n",
       "      <th>wurden</th>\n",
       "      <th>med</th>\n",
       "      <th>não</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>80.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>35.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amsterdam</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>61.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host</th>\n",
       "      <td>35.0</td>\n",
       "      <td>656.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reinhart</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cama</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lights</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estancia</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sander</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cool   nice  clean  quiet    use  didnt  finding   come   back  \\\n",
       "place      80.0  935.0  883.0  372.0  109.0    9.0     16.0  177.0  347.0   \n",
       "apartment  35.0  598.0  601.0  266.0   83.0    3.0      6.0  115.0  222.0   \n",
       "amsterdam   1.0   58.0   36.0   15.0    5.0    3.0      0.0   15.0   12.0   \n",
       "location   61.0  804.0  873.0  389.0  110.0   12.0      8.0  170.0  365.0   \n",
       "host       35.0  656.0  691.0  238.0   78.0    7.0      9.0  128.0  225.0   \n",
       "...         ...    ...    ...    ...    ...    ...      ...    ...    ...   \n",
       "reinhart    0.0    0.0    0.0    0.0    0.0    0.0      0.0    0.0    0.0   \n",
       "cama        0.0    0.0    0.0    0.0    0.0    0.0      0.0    0.0    1.0   \n",
       "lights      0.0    9.0    4.0    3.0    5.0    0.0      0.0    0.0    1.0   \n",
       "estancia    0.0    0.0    0.0    0.0    0.0    0.0      0.0    0.0    0.0   \n",
       "sander      0.0    0.0    0.0    0.0    0.0    0.0      0.0    0.0    0.0   \n",
       "\n",
       "           amazing  ...  wenn  schöne  wirklich  noch  diese  jeden  sie  \\\n",
       "place        306.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "apartment    187.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "amsterdam     12.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "location     305.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "host         237.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "...            ...  ...   ...     ...       ...   ...    ...    ...  ...   \n",
       "reinhart       0.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "cama           0.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "lights         5.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "estancia       0.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "sander         0.0  ...   0.0     0.0       0.0   0.0    0.0    0.0  0.0   \n",
       "\n",
       "           wurden  med  não  \n",
       "place         0.0  0.0  0.0  \n",
       "apartment     0.0  0.0  0.0  \n",
       "amsterdam     0.0  0.0  0.0  \n",
       "location      0.0  0.0  0.0  \n",
       "host          0.0  0.0  0.0  \n",
       "...           ...  ...  ...  \n",
       "reinhart      0.0  0.0  0.0  \n",
       "cama          0.0  0.0  4.0  \n",
       "lights        0.0  0.0  0.0  \n",
       "estancia      0.0  0.0  0.0  \n",
       "sander        0.0  0.0  0.0  \n",
       "\n",
       "[1000 rows x 998 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please note only the first 10000 entries from the df were taken on previous step due to excessively long computation times.\n",
    "\n",
    "coocdf = cooc_dict2df(coocs)\n",
    "print(coocdf.shape)\n",
    "\n",
    "coocdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EWllWryR-QL"
   },
   "source": [
    "### 3.a5 Raw co-occurrences to PMI scores\n",
    "\n",
    "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "frTTs7-eSFHv"
   },
   "outputs": [],
   "source": [
    "def cooc2pmi(df):\n",
    "  # your code here\n",
    "\n",
    "    '''\n",
    "    Converts a DataFrame of raw occurrences center word vs context word into a PMI score matrix.\n",
    "    PMI formula follows a frequentist probability calculation.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        DataFrame containing context words as columns, center words as rows. Values show the co-occurrence values.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        Transformed DataFrame into PMI score matrix.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # N is the sum of total occurrences in the DataFrame. This will be required to calculate probabilities later on.\n",
    "    N = df.to_numpy().sum()\n",
    "\n",
    "    # We calculate PMI scores column by column:\n",
    "    for column in tqdm(df.columns):\n",
    "    \n",
    "        # We follow the PMI formula calculating probabilities of:\n",
    "        # pw - center words in any context.\n",
    "        # pc - context words for any center word.\n",
    "        # pwc - joint appearances of each center and context word.\n",
    "        \n",
    "        # Being already in a fixed column, calculate PMI scores row by row following df.index (contains all rows):\n",
    "        for row in df.index:\n",
    "            pwc = ( df.loc[row, column] ) / N # x/y located value\n",
    "            pw = ( df.loc[row, :].sum() ) / N # sum of all values for this row\n",
    "            pc = ( df[column].sum() ) / N # sum of all values for this column\n",
    "        \n",
    "            # print('pwc, pw, pc: ', pwc, pw, pc) # Left for debugging\n",
    "        \n",
    "            # Exception handling for math domain. Handle values of zero and also convert negative results to zero.\n",
    "            if (pwc == 0) or (pw*pc == 0):\n",
    "                df.loc[row, column] = 0\n",
    "        \n",
    "            else:\n",
    "                try:\n",
    "                    pmi = math.log( (pwc / (pw*pc) ), 2)\n",
    "            \n",
    "                except:\n",
    "                    df.loc[row, column] = 0\n",
    "        \n",
    "                if pmi > 0:\n",
    "                    df.loc[row, column] = pmi\n",
    "                else:\n",
    "                    df.loc[row, column] = 0\n",
    "    \n",
    "\n",
    "    # As the above strategy was overwritting the input DataFrame (df) using the .loc method, we copy the buffer df into \n",
    "    # the return required output pmidf:\n",
    "    \n",
    "    pmidf = df\n",
    "    \n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "AGftXjXRSuQw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 998/998 [06:46<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>nice</th>\n",
       "      <th>clean</th>\n",
       "      <th>quiet</th>\n",
       "      <th>use</th>\n",
       "      <th>didnt</th>\n",
       "      <th>finding</th>\n",
       "      <th>come</th>\n",
       "      <th>back</th>\n",
       "      <th>amazing</th>\n",
       "      <th>...</th>\n",
       "      <th>wenn</th>\n",
       "      <th>schöne</th>\n",
       "      <th>wirklich</th>\n",
       "      <th>noch</th>\n",
       "      <th>diese</th>\n",
       "      <th>jeden</th>\n",
       "      <th>sie</th>\n",
       "      <th>wurden</th>\n",
       "      <th>med</th>\n",
       "      <th>não</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apartment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amsterdam</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.04118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reinhart</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cama</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.926649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.562796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lights</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.902002</td>\n",
       "      <td>3.429032</td>\n",
       "      <td>3.493542</td>\n",
       "      <td>4.278567</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.763578</td>\n",
       "      <td>4.051647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estancia</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sander</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cool      nice     clean     quiet       use    didnt  finding  \\\n",
       "place       0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "apartment   0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "amsterdam   0.0  0.000000  0.000000  0.000000  0.000000  3.04118      0.0   \n",
       "location    0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "host        0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "...         ...       ...       ...       ...       ...      ...      ...   \n",
       "reinhart    0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "cama        0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "lights      0.0  3.902002  3.429032  3.493542  4.278567  0.00000      0.0   \n",
       "estancia    0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "sander      0.0  0.000000  0.000000  0.000000  0.000000  0.00000      0.0   \n",
       "\n",
       "               come      back   amazing  ...  wenn  schöne  wirklich  noch  \\\n",
       "place      0.000000  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "apartment  0.000000  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "amsterdam  0.163855  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "location   0.000000  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "host       0.000000  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "...             ...       ...       ...  ...   ...     ...       ...   ...   \n",
       "reinhart   0.000000  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "cama       0.000000  5.926649  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "lights     0.000000  2.763578  4.051647  ...   0.0     0.0       0.0   0.0   \n",
       "estancia   0.000000  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "sander     0.000000  0.000000  0.000000  ...   0.0     0.0       0.0   0.0   \n",
       "\n",
       "           diese  jeden  sie  wurden  med       não  \n",
       "place        0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "apartment    0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "amsterdam    0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "location     0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "host         0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "...          ...    ...  ...     ...  ...       ...  \n",
       "reinhart     0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "cama         0.0    0.0  0.0     0.0  0.0  7.562796  \n",
       "lights       0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "estancia     0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "sander       0.0    0.0  0.0     0.0  0.0  0.000000  \n",
       "\n",
       "[1000 rows x 998 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please note only the first 10000 entries from the df were taken on previous steps due to excessively long computation times.\n",
    "\n",
    "pmidf = cooc2pmi(coocdf)\n",
    "print(pmidf.shape)\n",
    "\n",
    "pmidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaLRvjRySOYB"
   },
   "source": [
    "### 3.a6 Retrieve top-k context words, given a center word\n",
    "\n",
    "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "NlKUP9SgSXlL"
   },
   "outputs": [],
   "source": [
    "def topk(df, center_word, N=10):\n",
    "  # your code here\n",
    "\n",
    "    '''\n",
    "    Given a DataFrame in the way of a matrix of PMI scores of center words vs context words, returns the top-k context words\n",
    "    for a given center word.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        df: DataFrame in the way of a matrix of PMI scores of center words vs context words.\n",
    "            Columns are context words, Rows are center words.\n",
    "            \n",
    "        center_word: The word (row) from which we retrieve the top-k context words (columns).\n",
    "        \n",
    "        N: The number of context word we wish to retrieve, sorted by PMI score.\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        top_word[:N]: A list of strings, the top-N context words sorted by PMI score, for a given center word.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    top_words = list(df.loc[center_word, :].sort_values(ascending = False).index)\n",
    "\n",
    "    return top_words[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1I038zG1Sw62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy',\n",
       " 'complimentary',\n",
       " 'written',\n",
       " 'numerous',\n",
       " 'wide',\n",
       " 'waking',\n",
       " 'regular',\n",
       " 'older',\n",
       " 'delicious',\n",
       " 'save']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk(pmidf, 'coffee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfcm5-7b0HKO"
   },
   "source": [
    "# 3.b Ethical, social and legal implications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3uf-Qq4tYg"
   },
   "source": [
    "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
    "\n",
    "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
    "\n",
    "Your report should be between 500 and 750 words long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6QJyuP6I1Ht"
   },
   "source": [
    "### Your answer here. No Python, only Markdown.\n",
    "\n",
    "Write your answer after the line.\n",
    "\n",
    "---\n",
    "\n",
    "The main conflict with the framework would be with action 3 - comply with the law.\n",
    "\n",
    "Suggesting a price not in accordance with local regulations could end up in the user either disregarding the law or causing confussion, as he/she may not be aware of the regulations, considering the returned prices within the regular band.\n",
    "\n",
    "On both cases a system recommending prices above the maximum could result in users incited to post properties above the maximum legal price, causing the conflict.\n",
    "\n",
    "\n",
    "Fairness:\n",
    "\n",
    "These inflated prices effect could have more or less effect in the end-used depending on his/her Bias: For example, one user looking for properties who may be unaware of the event may tolerate aumented prices in big cities such as Barcelona, as he/she may not be familiar with the regular prices but could end up paying a higher price because of having heard that big touristic cities are way more expensie. However should this event/circumstance occurr in a small town, the user may investigate the reason bit further. \n",
    "\n",
    "From the ACM code of ethics, in terms of malfunctions and social consequences, an increase in price for requesting users could leave low-income-users stranded as they could not afford to get one property, and therefore those willing attend the event from outside but having a reduced budget would get excluded. Going beyond, charity associations or other well-intended movements/associations without profit spirit - who may want to be involved in the event - wouldn't be able to afford the properties either, therefore they couldn't participate in the even. This will result not just in an handicap for these associations, but also in the event itself, as we are preventing these sort of groups from participating and therefore having an impact on the attendance profile. It is critical that the algorithm is designed with good will: In \"Algorithms, governance and regulation: beyond 'the necessary hasthtags. Leighton Andrews, Cardiff Business School, Cardiff University, Wales\" is it discussed the harm of \"algorithmic lawbreaking\", where algorithms are intentionally designed to deceive lawmakers and regulators. The arise of these algorithms has lead to stronger regulations and inspections therefore we must ensure our project is well documented and showing the actual intention of the recommender system.\n",
    "\n",
    "\n",
    "Accountability:\n",
    "\n",
    "\n",
    "One protocol to ensure unfairness is eliminated, could be to introduce a final step before returning the final recommended price: Apart from checking the result against the local regulations, we could perform a t-test to compare the entries analyzed within the given timeframe with the rest of the data, or at least a bigger sample. Based on these considerations we could have two options: To max-out the maximum returned value when exceeding the regulations or to trigger a warning depending of the significance of the t-test results. This would be in accordance with to political initiatives already taken in cities such as Barcelona, to promote ethical, citizen-centric, data-driven policymaking (Calzada I. & Almirall, E. (2019) Barcelona's grassroots-led urban experimentation: Deciphering the 'data commons' policy scheme), where the Data commons initiative puts the citizen in the centre of the use of data-driven technologies, before bussiness related strategies.\n",
    "\n",
    "\n",
    "Transparency:\n",
    "\n",
    "As the warnings could be disregarded we would need to make the choice of returning the actual calculated value or the maxed-out. Following the article \"Evaluating Predictive Algorithms\" by David Demortain and Bilel Benbouzid, algorithms such as PredPol to predict crime, returned predictions are impossible to validate, for example, the algorith predicts an high crime-rate in a certain area, then because that predicted zone is then patrolled by the police, the predicted crimes do not happen, but not because of a fail in the prediction, but because of the preventive actions taken beforehand. (Algorithmic Regulation, The London School of Economics and Political Science. Discussion Paper No: 85, September 2017). Because of this, the algorithms, intentions and continous reviews of the code and implementation need to be clearly documented and communicated to the public, so in this way unexpected behaviours can be communicated and corrected.\n",
    "\n",
    "\n",
    "\n",
    "The solution to improve the system has already been discussed during the review of the above principles. The system could evaluate the returned results against a bigger sample and then either max-out the output comparing against a table of government regulations, of triggering a warning saying that the system has detected that the recommended price may exceed the maximum limit as per the local regulations and encouraging the user to consult the local authorities.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
